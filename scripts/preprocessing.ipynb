{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUyi0iR-Dr2w"
   },
   "source": [
    "## Create Pre-Processed Datasets and Vocabularies\n",
    "The goal of this file is to create data sets with pre-tokenized words and vocabularies than can be loaded into the iterators that will eventually feed our models.  Because our tokenizer of choice (spacy) is relatively slow, this process saves us enormous time.  Three datasets and four vocabularies are created in this file:\n",
    "\n",
    "Datasets\n",
    "1. small dataset for testing code\n",
    "2. full dataset\n",
    "3. dataset that Grasser et al used (which includes duplicates within and across the training and testing dataset)\n",
    "\n",
    "Datasets are pre-processed then split into train, valid, and test.  Details on pre-processing steps available below\n",
    "\n",
    "Vocabularies:\n",
    "1. Vocabulary from full training dataset\n",
    "2. Vocabulary of lemmatized full training set\n",
    "3. Vocabulary of Grasser's training dataset\n",
    "4. Vocabulary of lemmatized Grasser's training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: this file runs faster locally than on google colab - go figure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjZDsQHvEZ25"
   },
   "source": [
    "## Setup Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWU7OztTHUON"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bSq-NWzZHSR_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from csv import reader\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "Pre-processing involves the following steps:\n",
    "1. Drop examples with no rating.  Rating is our target so we cannot use examples without that information.\n",
    "2. Make conditional coluns into strings.  We did not use the condition field during our analysis.  However, this could be important for future work.\n",
    "3. Drop duplicates.  In some cases there are review duplicates associated with multiple conditions in different rows.  In these instances the rows are compressed to one and the conditions are made into a list.  Note: For grassers dataset duplicates are not dropped in order to maintain an apples to apples comparison.\n",
    "4. Replacing drug names with a token THISDRUG.  This allows the recurrent network to differentiate between the drug that is the subject of the review (if the drug is spelled correctly) and other drugs that are mentioned as comparisons.\n",
    "5. Execute data cleaning steps.  These include:\n",
    "    - stripping quotations that are wrapping the reviews\n",
    "    - replacing html symbols with the corresponding punctuation\n",
    "\n",
    "\n",
    "6. Creating the rating category.  This bins the rating column according to the follwoing rules:\n",
    "\n",
    "    |Ratings | Bin|\n",
    "    |--------|----|\n",
    "    |0 - 3   | Negative|\n",
    "    |4 - 6   | Neutral|\n",
    "    |7 - 10  | Positive|\n",
    "    \n",
    "7. Tokenize review column.  The spacy tokenizer creates numerous useful fields from text.  These include parts of speech, shape of the word (for example lengths and capitalizations), and lemmas (there are others but these are the ones we kept).  We decided to stringfigy the list of tokens for each review and create a column out of those.  Thus, for each example we'd have one column that is a stringified list of the parts of speech for each word, one columne that is a stringified list of the lemmas for each word, etc.\n",
    "8. We also created one hot encodings for each column created from the tokenized data.  For example, the afformentioned stringified list of parts of speech would be transformed a stringified list of one hot encodings of all parts of speech found in the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0EYPCnNbWA0"
   },
   "source": [
    "## Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tbJYlZ4-Z5rj"
   },
   "outputs": [],
   "source": [
    "def handle_dups(df):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe\n",
    "    returns: pandas dataframe\n",
    "    This drops duplicates for reviews\n",
    "    For the few situations where the same review is associated with\n",
    "    multiple conditions, this concatenates the conditions with a '~'\n",
    "    \"\"\"\n",
    "    print(f\"len before drop: {len(df)}\")\n",
    "    new_df = df.drop_duplicates(['review', 'condition'])\n",
    "    new_df.groupby('review')['condition'].apply(lambda x: '~'.join(x)).reset_index()\n",
    "    print(f\"len after drop: {len(new_df)}\")\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRRNfB3jJ8no"
   },
   "source": [
    "## Creating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JczqAlLkJ8nq"
   },
   "outputs": [],
   "source": [
    "def create_cols(df):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe\n",
    "    returns: pandas dataframe\n",
    "    \n",
    "    bins rating column and interpolates empty conditions with 'Not Entered'\n",
    "    \"\"\"\n",
    "\n",
    "    # code NAs as a \"Not Entered\" category\n",
    "    df.loc[df['condition'].isna(), 'condition'] = 'Not Entered'\n",
    "\n",
    "    # creates ratings category by binning ratings\n",
    "    df['rating_category'] = 'Positive'\n",
    "    df.loc[df['rating'] < 7, 'rating_category'] = 'Neutral'\n",
    "    df.loc[df['rating'] < 4, 'rating_category'] = 'Negative'\n",
    "\n",
    "    # # create daily useful count\n",
    "    # max_date = df['date'].max()\n",
    "    # df['useful_daily'] = df['usefulCount'] / ((max_date - df['date']).dt.days + 1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVz7ENI2Jh-h"
   },
   "source": [
    "## Clean Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fnof6pcxmO13"
   },
   "outputs": [],
   "source": [
    "def unwrap_quotes(rev):\n",
    "    \"\"\"\n",
    "    rev (str): a review\n",
    "    returns the same review without the quotes around it\n",
    "    \"\"\"\n",
    "    if rev[-1] == '\"' and rev[0] == '\"':\n",
    "        return rev[1:-1]\n",
    "    else:\n",
    "        return rev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VyIH1MBhJmaK"
   },
   "outputs": [],
   "source": [
    "def strip_quotes(df):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe\n",
    "    returns: pandas dataframe\n",
    "    applies unwrap quotes to the dataframe\n",
    "    (i know it's a poorly named function)\n",
    "    \"\"\"\n",
    "    df['review'] = df['review'].apply(unwrap_quotes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "o6tYDb1qJ8nu"
   },
   "outputs": [],
   "source": [
    "def replace_html(df):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe\n",
    "    returns: pandas dataframe\n",
    "    \n",
    "    replaces html representation of an apostrophe with an apostrophe\n",
    "    \"\"\"\n",
    "    df['review'] = df['review'].str.replace(\"&#039;\", \"'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "114Y6yXBlRmi"
   },
   "outputs": [],
   "source": [
    "def all_cleaning_steps(df):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe\n",
    "    returns: pandas dataframe\n",
    "    \n",
    "    executes all (both) cleaning steps\n",
    "    \"\"\"\n",
    "    df = strip_quotes(df)\n",
    "    df = replace_html(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PgcQD6T1XTb"
   },
   "source": [
    "## Tokenized Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KNjB7L-T1dNW"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie5WV40hDOEl"
   },
   "outputs": [],
   "source": [
    "def make_tokens(review, idx, nlp):\n",
    "    \"\"\"\n",
    "    review (str): a review\n",
    "    idx (int): index (used for checking progress)\n",
    "    nlp: spacy object - the object returned from spacy.load()\n",
    "    \n",
    "    tokenizes review and returns a tuple of jsonfied lists of \n",
    "    token data.  currently takes the text, lemma, two parts of \n",
    "    speech, and the shape\n",
    "    \"\"\"\n",
    "#     print(\"idx\", idx)\n",
    "\n",
    "    if not idx % 10000:\n",
    "        print(f\"\\tokenizing review {idx}\")\n",
    "    # print(\"review\", review)\n",
    "    doc = nlp(review)\n",
    "    token_data = list(zip(*[(token.text, token.lemma_, token.pos_, \n",
    "              token.shape_, token.dep_) for token in doc]))\n",
    "    # print(token_data)\n",
    "    return (json.dumps(data) for data in token_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Rrda2O1cC04f"
   },
   "outputs": [],
   "source": [
    "def create_spacy_cols(df, nlp):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe\n",
    "    nlp: spacy object - the object returned from spacy.load()\n",
    "    returns: pandas dataframe\n",
    "    \n",
    "    applies make tokens to pandas dataframe and configures output to make sense\n",
    "    \n",
    "    \"\"\"\n",
    "    (df['tokens'], df['lemmas'], df['pos'], df['shape'], df['dep']) = \\\n",
    "        zip(*df.apply(lambda x: make_tokens(x['review'], x.name, nlp), axis=1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['condition', 'review', 'rating', 'date', 'usefulCount',\n",
       "       'rating_category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC-QeCst0jqA"
   },
   "source": [
    "### One hot cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jQpFhj7U3ihy"
   },
   "outputs": [],
   "source": [
    "def get_indices(df, col):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe\n",
    "    col: name of column\n",
    "    returns: list of unique values in col\n",
    "    \"\"\"\n",
    "    lists = list(df[col].apply(json.loads))\n",
    "    flat_list = [item for sublist in lists for item in sublist]\n",
    "    indices = list(set(flat_list))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Wy0Qu97m6NJ4"
   },
   "outputs": [],
   "source": [
    "def encode_col(s, indices):\n",
    "    \"\"\"\n",
    "    s: stringified list of tokens (they can be parts of speech, shapes, etc) \n",
    "    indices: list of unique values in a column\n",
    "    returns same thing as s but with tokens replaced with indices.\n",
    "    \"\"\"\n",
    "    l = json.loads(s)\n",
    "    new_l = [indices.index(i) for i in l]\n",
    "    return json.dumps(new_l)\n",
    "\n",
    "def one_hot_index(df, col):\n",
    "    \"\"\"\n",
    "    df: pandas dataframe\n",
    "    col: name of column\n",
    "    returns: pandas df with one hot column\n",
    "    \"\"\"\n",
    "    indices = get_indices(df, col)\n",
    "    df[col + \"_encoding\"] = df[col].apply(lambda x: encode_col(x,  indices))\n",
    "    df[col + \"_encoding_count\"] = len(indices) - 1\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EEoDbAN83ZV"
   },
   "source": [
    "## Replace drug name in review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WRrjXPou7iJM"
   },
   "outputs": [],
   "source": [
    "def replace_drug_name(row):\n",
    "    \"\"\"\n",
    "    row: row from pandas df (function is meant to be applied)\n",
    "    returns row 'THISDRUG' replacing the drugname field\n",
    "    \"\"\"\n",
    "    drug_name = row['drugName']\n",
    "    pattern = re.compile(drug_name, re.IGNORECASE)\n",
    "    row['review'] = pattern.sub('THISDRUG', row['review'])\n",
    "    return row\n",
    "\n",
    "def replace_drug_names(df):\n",
    "    \"\"\"\n",
    "    applies replace_drug_name to df\n",
    "    \"\"\"\n",
    "    df = df.apply(replace_drug_name, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNmZ2COwl8eK"
   },
   "source": [
    "## All steps Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8l94YbWGl7eO"
   },
   "outputs": [],
   "source": [
    "def execute_preprocessing(df):\n",
    "    \"\"\"\n",
    "    executes all functions above to df\n",
    "    returns transformed dataframe\n",
    "    \"\"\"\n",
    "    print(\"dropping null ratings...\")\n",
    "    df = df[df['rating'].notna()]\n",
    "    print(\"making condition columns str...\")\n",
    "    df['condition'] = df['condition'].astype(str)\n",
    "    print(\"handling duplicates...\")\n",
    "    df = handle_dups(df)\n",
    "    print(\"executing data cleaning steps...\")\n",
    "    df = all_cleaning_steps(df)\n",
    "    print(\"replacing drug names...\")\n",
    "    df = replace_drug_names(df)\n",
    "    print(\"creating rating category...\")\n",
    "    df = create_cols(df)\n",
    "    print(\"making spacy token columns..\")\n",
    "    df = create_spacy_cols(df, nlp)\n",
    "    print(\"creating one hot encording columns...\")\n",
    "    cols_to_encode = ['pos', 'shape', 'dep']\n",
    "    for col in cols_to_encode:\n",
    "        df = one_hot_index(df, col)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VQrEhWnGyQ5"
   },
   "source": [
    "## Create Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xACOaj90HcJx"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DOIr2cT2HbE9"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./drive-download-20210514T232544Z-001\"\n",
    "\n",
    "test = DATA_PATH + \"/drugsComTest_raw.tsv\"\n",
    "train = DATA_PATH + \"/drugsComTrain_raw.tsv\"\n",
    "scraped = DATA_PATH + \"/drugComScrapedData.tsv\"\n",
    " \n",
    "\n",
    "scraped_df = pd.read_csv(scraped, sep=\"\\t\")\n",
    "test_df = pd.read_csv(test, sep=\"\\t\", index_col=0)\n",
    "train_df = pd.read_csv(train, sep=\"\\t\", index_col=0)\n",
    "\n",
    "full_df = pd.concat([scraped_df, test_df, train_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qx5-Oo97KG83"
   },
   "source": [
    "## Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "iQ4XsdoRIMng"
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_dfs(df):\n",
    "    \"\"\"\n",
    "    creates train, valid, and test dfs\n",
    "    valid is stratified along label\n",
    "    \"\"\"\n",
    "    RANDOM_STATE = 1123\n",
    "    train, test = train_test_split(df, train_size=.85, random_state=RANDOM_STATE)\n",
    "    train, valid = train_test_split(train, \n",
    "                                    train_size=.85, \n",
    "                                    random_state=RANDOM_STATE, \n",
    "                                    stratify=train['rating_category'])\n",
    "    return train, valid, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "pa_Y9_V7N2xh"
   },
   "outputs": [],
   "source": [
    "def save_splits(root, name, train, valid, test):\n",
    "    \"\"\"\n",
    "    saves train, valid, and test dfs\n",
    "    \"\"\"\n",
    "    train.to_csv(f\"{root}/train_{name}.csv\", index=False)\n",
    "    valid.to_csv(f\"{root}/valid_{name}.csv\", index=False)\n",
    "    test.to_csv(f\"{root}/test_{name}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6zB0FgULaKq"
   },
   "source": [
    "rma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8FqYa7SC8Sa"
   },
   "source": [
    "## Make New DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "m0cRE9djLk3_",
    "outputId": "80955919-c3b6-4f34-adab-db40db0c97bf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping null ratings...\n",
      "making condition columns str...\n",
      "handling duplicates...\n",
      "len before drop: 48\n",
      "len after drop: 48\n",
      "executing data cleaning steps...\n",
      "replacing drug names...\n",
      "creating rating category...\n",
      "making spacy token columns..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-156-2626f68cfa84>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['condition'] = df['condition'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating one hot encording columns...\n",
      "dropping null ratings...\n",
      "making condition columns str...\n",
      "handling duplicates...\n",
      "len before drop: 421480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-156-2626f68cfa84>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['condition'] = df['condition'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len after drop: 181540\n",
      "executing data cleaning steps...\n",
      "replacing drug names...\n",
      "creating rating category...\n",
      "making spacy token columns..\n",
      "\tokenizing review 0\n",
      "\tokenizing review 20000\n",
      "\tokenizing review 30000\n",
      "\tokenizing review 50000\n",
      "\tokenizing review 70000\n",
      "\tokenizing review 110000\n",
      "\tokenizing review 130000\n",
      "\tokenizing review 230000\n",
      "\tokenizing review 240000\n",
      "\tokenizing review 250000\n",
      "\tokenizing review 260000\n",
      "\tokenizing review 270000\n",
      "\tokenizing review 280000\n",
      "\tokenizing review 310000\n",
      "\tokenizing review 330000\n",
      "\tokenizing review 350000\n",
      "\tokenizing review 370000\n",
      "\tokenizing review 410000\n",
      "\tokenizing review 430000\n",
      "creating one hot encording columns...\n"
     ]
    }
   ],
   "source": [
    "# sample for testing\n",
    "small_df = full_df.sample(n=50, random_state=6624)\n",
    "small_df_pp = execute_preprocessing(small_df)\n",
    "save_splits(f\"{DATA_PATH}/full_processed\", 'small_df', *split_dfs(small_df_pp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full df\n",
    "full_df_pp = execute_preprocessing(full_df)\n",
    "full_df_pp.to_csv(f\"{DATA_PATH}/full_processed/full_df.csv\", index=False)\n",
    "save_splits(f\"{DATA_PATH}/full_processed\", 'full_df', *split_dfs(full_df_pp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process pure Grasser data\n",
    "Alternative versions of functions because Grasser data is already split into train-test we shouldn't remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dfs_valid(df):\n",
    "    \"\"\"\n",
    "    creates train, valid, and test dfs\n",
    "    valid is stratified along label\n",
    "    \"\"\"\n",
    "    RANDOM_STATE = 1123\n",
    "    train, valid = train_test_split(df, train_size=.85, random_state=RANDOM_STATE)\n",
    "    return train, valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_splits_valid(root, name, train, valid):\n",
    "    \"\"\"\n",
    "    saves train, valid, and test dfs\n",
    "    \"\"\"\n",
    "    train.to_csv(f\"{root}/train_{name}.csv\", index=False)\n",
    "    valid.to_csv(f\"{root}/valid_{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_preprocessing_keep_dups(df):\n",
    "    \"\"\"\n",
    "    executes all functions above to df\n",
    "    returns transformed dataframe\n",
    "    \"\"\"\n",
    "    print(\"dropping null ratings...\")\n",
    "    df = df[df['rating'].notna()]\n",
    "    print(\"making condition columns str...\")\n",
    "    df['condition'] = df['condition'].astype(str)\n",
    "#     print(\"handling duplicates...\")\n",
    "#     df = handle_dups(df)\n",
    "    print(\"executing data cleaning steps...\")\n",
    "    df = all_cleaning_steps(df)\n",
    "    print(\"replacing drug names...\")\n",
    "    df = replace_drug_names(df)\n",
    "    print(\"creating rating category...\")\n",
    "    df = create_cols(df)\n",
    "    print(\"making spacy token columns..\")\n",
    "    df = create_spacy_cols(df, nlp)\n",
    "    print(\"creating one hot encording columns...\")\n",
    "    cols_to_encode = ['pos', 'shape', 'dep']\n",
    "    for col in cols_to_encode:\n",
    "        df = one_hot_index(df, col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['drugName', 'condition', 'review', 'rating', 'date', 'usefulCount',\n",
      "       'rating_category'],\n",
      "      dtype='object')\n",
      "dropping null ratings...\n",
      "making condition columns str...\n",
      "executing data cleaning steps...\n",
      "replacing drug names...\n",
      "creating rating category...\n",
      "making spacy token columns..\n",
      "\tokenizing review 0\n",
      "\tokenizing review 10000\n",
      "\tokenizing review 20000\n",
      "\tokenizing review 30000\n",
      "\tokenizing review 40000\n",
      "\tokenizing review 50000\n",
      "\tokenizing review 60000\n",
      "\tokenizing review 70000\n",
      "\tokenizing review 80000\n",
      "\tokenizing review 90000\n",
      "\tokenizing review 100000\n",
      "\tokenizing review 110000\n",
      "\tokenizing review 120000\n",
      "\tokenizing review 130000\n",
      "\tokenizing review 140000\n",
      "\tokenizing review 150000\n",
      "\tokenizing review 160000\n",
      "creating one hot encording columns...\n",
      "dropping null ratings...\n",
      "making condition columns str...\n",
      "executing data cleaning steps...\n",
      "replacing drug names...\n",
      "creating rating category...\n",
      "making spacy token columns..\n",
      "\tokenizing review 0\n",
      "\tokenizing review 10000\n",
      "\tokenizing review 20000\n",
      "\tokenizing review 30000\n",
      "\tokenizing review 40000\n",
      "\tokenizing review 50000\n",
      "creating one hot encording columns...\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"./grasser_data/\"\n",
    "\n",
    "test = DATA_PATH + \"test_original.csv\"\n",
    "train = DATA_PATH + \"train_original.csv\"\n",
    "test_df = pd.read_csv(test)\n",
    "train_df = pd.read_csv(train)\n",
    "print(train_df.columns)\n",
    "\n",
    "grasser_train_processed = execute_preprocessing_keep_dups(train_df)\n",
    "save_splits_valid(f\"{DATA_PATH}\", 'grasser_data', *split_dfs_valid(grasser_train_processed))\n",
    "grasser_test_processed = execute_preprocessing_keep_dups(test_df)\n",
    "grasser_test_processed.to_csv(f\"{DATA_PATH}/test_grasser_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(csv_file, counter, tokenizer, min_freq):\n",
    "    \"\"\"\n",
    "    creates pytorch vocab utility\n",
    "    \n",
    "    csv_file: string, self exlanatory\n",
    "    counter: counter object (should be empty)\n",
    "    tokenizer: tokenizer object\n",
    "    min_freq: int (minimum frequency)\n",
    "    \n",
    "    returns vocab object\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        csv_reader = reader(f)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            tokens = tokenizer(row[2].lower())\n",
    "            counter.update(tokens)\n",
    "            if not i % 10000:\n",
    "                print(f\"{i} examples completed\")\n",
    "    vocab_reviews = Vocab(counter_reviews, min_freq=min_freq)\n",
    "    return vocab_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 examples completed\n",
      "10000 examples completed\n",
      "20000 examples completed\n",
      "30000 examples completed\n",
      "40000 examples completed\n",
      "50000 examples completed\n",
      "60000 examples completed\n",
      "70000 examples completed\n",
      "80000 examples completed\n",
      "90000 examples completed\n",
      "100000 examples completed\n",
      "110000 examples completed\n",
      "120000 examples completed\n",
      "130000 examples completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "counter_reviews = Counter()\n",
    "\n",
    "review_vocab = create_vocab(f\"{DATA_PATH}/full_processed/train_full_df.csv\", counter_reviews,  tokenizer, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = json.dumps(counter_reviews)\n",
    "with open('full_vocab.json', 'w') as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab for Grasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_reviews = Counter()\n",
    "review_vocab = create_vocab(f\"{DATA_PATH}/grasser_data/train_grasser_data.csv\", counter_reviews,  tokenizer, 100)\n",
    "s = json.dumps(counter_reviews)\n",
    "with open('grasser_vocab.json', 'w') as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab for lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "counter_reviews = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_lemma(csv_file, counter, tokenizer, min_freq):\n",
    "    \"\"\"\n",
    "    creates pytorch vocab utility\n",
    "    \n",
    "    csv_file: string, self exlanatory\n",
    "    counter: counter object (should be empty)\n",
    "    tokenizer: tokenizer object\n",
    "    min_freq: int (minimum frequency)\n",
    "    \n",
    "    returns vocab object    \n",
    "    \"\"\"\n",
    "    with open(csv_file, 'r') as f:\n",
    "        csv_reader = reader(f)\n",
    "        \n",
    "        for i, row in enumerate(csv_reader):\n",
    "            doc = nlp(row[2].lower())\n",
    "            lemmas = [token.lemma_ for token in doc]\n",
    "#             print(\"token\", type(token))\n",
    "#             print(\"lemma\", lemmas)\n",
    "            counter.update(lemmas)\n",
    "#             if i == 2: break\n",
    "            if not i % 10000:\n",
    "                print(f\"{i} examples completed\")\n",
    "    vocab_reviews = Vocab(counter_reviews, min_freq=min_freq)\n",
    "    return vocab_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 examples completed\n",
      "10000 examples completed\n",
      "20000 examples completed\n",
      "30000 examples completed\n",
      "40000 examples completed\n",
      "50000 examples completed\n",
      "60000 examples completed\n",
      "70000 examples completed\n",
      "80000 examples completed\n",
      "90000 examples completed\n",
      "100000 examples completed\n",
      "110000 examples completed\n",
      "120000 examples completed\n",
      "130000 examples completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemma_vocab = create_vocab_lemma(f\"{DATA_PATH}/full_processed/train_full_df.csv\", \n",
    "                            counter_reviews, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7ff8c28cc6a0>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'I': 2,\n",
       "             '.': 3,\n",
       "             'be': 4,\n",
       "             'and': 5,\n",
       "             ',': 6,\n",
       "             'the': 7,\n",
       "             'have': 8,\n",
       "             'to': 9,\n",
       "             'it': 10,\n",
       "             'my': 11,\n",
       "             'a': 12,\n",
       "             'for': 13,\n",
       "             'of': 14,\n",
       "             ' ': 15,\n",
       "             'this': 16,\n",
       "             'take': 17,\n",
       "             'on': 18,\n",
       "             'in': 19,\n",
       "             'do': 20,\n",
       "             'but': 21,\n",
       "             'that': 22,\n",
       "             'with': 23,\n",
       "             'day': 24,\n",
       "             '!': 25,\n",
       "             'not': 26,\n",
       "             \"n't\": 27,\n",
       "             'so': 28,\n",
       "             'get': 29,\n",
       "             'go': 30,\n",
       "             'feel': 31,\n",
       "             'thisdrug': 32,\n",
       "             'month': 33,\n",
       "             'at': 34,\n",
       "             'year': 35,\n",
       "             'after': 36,\n",
       "             'work': 37,\n",
       "             'as': 38,\n",
       "             'no': 39,\n",
       "             'start': 40,\n",
       "             'week': 41,\n",
       "             'effect': 42,\n",
       "             'now': 43,\n",
       "             'side': 44,\n",
       "             'about': 45,\n",
       "             'time': 46,\n",
       "             'all': 47,\n",
       "             'pain': 48,\n",
       "             'mg': 49,\n",
       "             '-': 50,\n",
       "             ')': 51,\n",
       "             'you': 52,\n",
       "             'up': 53,\n",
       "             'bad': 54,\n",
       "             '(': 55,\n",
       "             'like': 56,\n",
       "             'use': 57,\n",
       "             'well': 58,\n",
       "             'first': 59,\n",
       "             'only': 60,\n",
       "             \"'ve\": 61,\n",
       "             'just': 62,\n",
       "             'from': 63,\n",
       "             'pill': 64,\n",
       "             'out': 65,\n",
       "             'or': 66,\n",
       "             'very': 67,\n",
       "             '2': 68,\n",
       "             'if': 69,\n",
       "             'try': 70,\n",
       "             'would': 71,\n",
       "             'when': 72,\n",
       "             'they': 73,\n",
       "             'make': 74,\n",
       "             '3': 75,\n",
       "             'help': 76,\n",
       "             'medication': 77,\n",
       "             'period': 78,\n",
       "             'back': 79,\n",
       "             'one': 80,\n",
       "             'doctor': 81,\n",
       "             'because': 82,\n",
       "             'more': 83,\n",
       "             'can': 84,\n",
       "             'also': 85,\n",
       "             'will': 86,\n",
       "             'before': 87,\n",
       "             'then': 88,\n",
       "             'good': 89,\n",
       "             'an': 90,\n",
       "             'give': 91,\n",
       "             'which': 92,\n",
       "             'stop': 93,\n",
       "             'anxiety': 94,\n",
       "             'sleep': 95,\n",
       "             'life': 96,\n",
       "             'experience': 97,\n",
       "             'still': 98,\n",
       "             'off': 99,\n",
       "             'other': 100,\n",
       "             'could': 101,\n",
       "             'since': 102,\n",
       "             'any': 103,\n",
       "             'weight': 104,\n",
       "             'hour': 105,\n",
       "             'over': 106,\n",
       "             'never': 107,\n",
       "             'drug': 108,\n",
       "             'some': 109,\n",
       "             'much': 110,\n",
       "             'really': 111,\n",
       "             'medicine': 112,\n",
       "             'think': 113,\n",
       "             'night': 114,\n",
       "             ';': 115,\n",
       "             'last': 116,\n",
       "             '&': 117,\n",
       "             'than': 118,\n",
       "             'say': 119,\n",
       "             'control': 120,\n",
       "             'two': 121,\n",
       "             '4': 122,\n",
       "             'every': 123,\n",
       "             'even': 124,\n",
       "             'dose': 125,\n",
       "             '5': 126,\n",
       "             'again': 127,\n",
       "             'few': 128,\n",
       "             'by': 129,\n",
       "             'prescribe': 130,\n",
       "             '10': 131,\n",
       "             'long': 132,\n",
       "             'great': 133,\n",
       "             'gain': 134,\n",
       "             'ago': 135,\n",
       "             'acne': 136,\n",
       "             'thing': 137,\n",
       "             'see': 138,\n",
       "             'he': 139,\n",
       "             'what': 140,\n",
       "             'know': 141,\n",
       "             'come': 142,\n",
       "             'lose': 143,\n",
       "             'put': 144,\n",
       "             'find': 145,\n",
       "             '6': 146,\n",
       "             'depression': 147,\n",
       "             'headache': 148,\n",
       "             'little': 149,\n",
       "             'severe': 150,\n",
       "             'there': 151,\n",
       "             'down': 152,\n",
       "             '/': 153,\n",
       "             'change': 154,\n",
       "             'want': 155,\n",
       "             'birth': 156,\n",
       "             'your': 157,\n",
       "             'recommend': 158,\n",
       "             'almost': 159,\n",
       "             'problem': 160,\n",
       "             'lot': 161,\n",
       "             'need': 162,\n",
       "             'skin': 163,\n",
       "             'cause': 164,\n",
       "             '1': 165,\n",
       "             'eat': 166,\n",
       "             'symptom': 167,\n",
       "             'tell': 168,\n",
       "             '...': 169,\n",
       "             'ever': 170,\n",
       "             'nothing': 171,\n",
       "             'cramp': 172,\n",
       "             'notice': 173,\n",
       "             'away': 174,\n",
       "             'mood': 175,\n",
       "             'n’t': 176,\n",
       "             'nausea': 177,\n",
       "             'too': 178,\n",
       "             'far': 179,\n",
       "             'keep': 180,\n",
       "             'she': 181,\n",
       "             'ca': 182,\n",
       "             'body': 183,\n",
       "             'morning': 184,\n",
       "             'med': 185,\n",
       "             'these': 186,\n",
       "             'seem': 187,\n",
       "             '\\n\\n': 188,\n",
       "             'normal': 189,\n",
       "             'without': 190,\n",
       "             'hope': 191,\n",
       "             '20': 192,\n",
       "             'while': 193,\n",
       "             'old': 194,\n",
       "             'horrible': 195,\n",
       "             'how': 196,\n",
       "             'until': 197,\n",
       "             'clear': 198,\n",
       "             'migraine': 199,\n",
       "             'low': 200,\n",
       "             'however': 201,\n",
       "             'next': 202,\n",
       "             '\\n': 203,\n",
       "             'switch': 204,\n",
       "             'within': 205,\n",
       "             'many': 206,\n",
       "             'once': 207,\n",
       "             'stomach': 208,\n",
       "             'blood': 209,\n",
       "             'around': 210,\n",
       "             'sex': 211,\n",
       "             'through': 212,\n",
       "             'its': 213,\n",
       "             'most': 214,\n",
       "             'suffer': 215,\n",
       "             'wake': 216,\n",
       "             'due': 217,\n",
       "             'we': 218,\n",
       "             'issue': 219,\n",
       "             'always': 220,\n",
       "             'later': 221,\n",
       "             'right': 222,\n",
       "             'high': 223,\n",
       "             'increase': 224,\n",
       "             'love': 225,\n",
       "             'able': 226,\n",
       "             'same': 227,\n",
       "             'finally': 228,\n",
       "             'amp': 229,\n",
       "             'read': 230,\n",
       "             'people': 231,\n",
       "             'myself': 232,\n",
       "             'second': 233,\n",
       "             'different': 234,\n",
       "             'three': 235,\n",
       "             'way': 236,\n",
       "             '?': 237,\n",
       "             '8': 238,\n",
       "             'happy': 239,\n",
       "             'review': 240,\n",
       "             'sure': 241,\n",
       "             'who': 242,\n",
       "             'daily': 243,\n",
       "             'attack': 244,\n",
       "             '  ': 245,\n",
       "             '30': 246,\n",
       "             '7': 247,\n",
       "             'face': 248,\n",
       "             'anything': 249,\n",
       "             'another': 250,\n",
       "             'less': 251,\n",
       "             'something': 252,\n",
       "             'completely': 253,\n",
       "             'pound': 254,\n",
       "             'during': 255,\n",
       "             'infection': 256,\n",
       "             'become': 257,\n",
       "             'drink': 258,\n",
       "             'into': 259,\n",
       "             'though': 260,\n",
       "             'dry': 261,\n",
       "             '..': 262,\n",
       "             'bleed': 263,\n",
       "             'result': 264,\n",
       "             'end': 265,\n",
       "             'spot': 266,\n",
       "             'light': 267,\n",
       "             'dr': 268,\n",
       "             'look': 269,\n",
       "             'couple': 270,\n",
       "             'new': 271,\n",
       "             'half': 272,\n",
       "             'minute': 273,\n",
       "             'panic': 274,\n",
       "             \"'s\": 275,\n",
       "             '’m': 276,\n",
       "             'stay': 277,\n",
       "             'everything': 278,\n",
       "             'relief': 279,\n",
       "             'begin': 280,\n",
       "             'terrible': 281,\n",
       "             'tired': 282,\n",
       "             'drive': 283,\n",
       "             'decide': 284,\n",
       "             'bit': 285,\n",
       "             'worth': 286,\n",
       "             'swing': 287,\n",
       "             ':': 288,\n",
       "             'bed': 289,\n",
       "             'may': 290,\n",
       "             'break': 291,\n",
       "             '15': 292,\n",
       "             'heavy': 293,\n",
       "             'extremely': 294,\n",
       "             'water': 295,\n",
       "             'past': 296,\n",
       "             'painful': 297,\n",
       "             'today': 298,\n",
       "             'where': 299,\n",
       "             'definitely': 300,\n",
       "             'treatment': 301,\n",
       "             'product': 302,\n",
       "             'loss': 303,\n",
       "             'shot': 304,\n",
       "             'actually': 305,\n",
       "             'continue': 306,\n",
       "             'pretty': 307,\n",
       "             'effective': 308,\n",
       "             'hard': 309,\n",
       "             'twice': 310,\n",
       "             'should': 311,\n",
       "             'feeling': 312,\n",
       "             'diagnose': 313,\n",
       "             'appetite': 314,\n",
       "             '\"': 315,\n",
       "             'heart': 316,\n",
       "             'mouth': 317,\n",
       "             '100': 318,\n",
       "             'leave': 319,\n",
       "             '50': 320,\n",
       "             '’ve': 321,\n",
       "             'pressure': 322,\n",
       "             '%': 323,\n",
       "             'injection': 324,\n",
       "             'happen': 325,\n",
       "             'eye': 326,\n",
       "             'full': 327,\n",
       "             'several': 328,\n",
       "             'fall': 329,\n",
       "             'anyone': 330,\n",
       "             'everyone': 331,\n",
       "             'fine': 332,\n",
       "             'sometimes': 333,\n",
       "             'insurance': 334,\n",
       "             'bleeding': 335,\n",
       "             'burn': 336,\n",
       "             'point': 337,\n",
       "             'pregnant': 338,\n",
       "             'dosage': 339,\n",
       "             'leg': 340,\n",
       "             'enough': 341,\n",
       "             'sick': 342,\n",
       "             'big': 343,\n",
       "             'taste': 344,\n",
       "             'else': 345,\n",
       "             'both': 346,\n",
       "             'head': 347,\n",
       "             'awful': 348,\n",
       "             'food': 349,\n",
       "             'diarrhea': 350,\n",
       "             'level': 351,\n",
       "             'energy': 352,\n",
       "             'soon': 353,\n",
       "             'those': 354,\n",
       "             'small': 355,\n",
       "             'insomnia': 356,\n",
       "             '12': 357,\n",
       "             'hair': 358,\n",
       "             'wait': 359,\n",
       "             'mild': 360,\n",
       "             'call': 361,\n",
       "             \"'ll\": 362,\n",
       "             'patch': 363,\n",
       "             'hurt': 364,\n",
       "             'least': 365,\n",
       "             'already': 366,\n",
       "             'depressed': 367,\n",
       "             '$': 368,\n",
       "             'negative': 369,\n",
       "             'maybe': 370,\n",
       "             'deal': 371,\n",
       "             \"'\": 372,\n",
       "             'extreme': 373,\n",
       "             'amazing': 374,\n",
       "             'difference': 375,\n",
       "             'muscle': 376,\n",
       "             'here': 377,\n",
       "             '25': 378,\n",
       "             'absolutely': 379,\n",
       "             'affect': 380,\n",
       "             'immediately': 381,\n",
       "             'walk': 382,\n",
       "             'test': 383,\n",
       "             'tablet': 384,\n",
       "             'thank': 385,\n",
       "             \"'d\": 386,\n",
       "             'free': 387,\n",
       "             'anymore': 388,\n",
       "             'usually': 389,\n",
       "             'per': 390,\n",
       "             'thought': 391,\n",
       "             'recently': 392,\n",
       "             'ask': 393,\n",
       "             'cry': 394,\n",
       "             'overall': 395,\n",
       "             '9': 396,\n",
       "             'why': 397,\n",
       "             'surgery': 398,\n",
       "             '40': 399,\n",
       "             'pm': 400,\n",
       "             'cold': 401,\n",
       "             'believe': 402,\n",
       "             'hand': 403,\n",
       "             'whole': 404,\n",
       "             'exercise': 405,\n",
       "             'cream': 406,\n",
       "             'sore': 407,\n",
       "             'live': 408,\n",
       "             'wonder': 409,\n",
       "             'each': 410,\n",
       "             'reduce': 411,\n",
       "             'etc': 412,\n",
       "             'short': 413,\n",
       "             'prescription': 414,\n",
       "             '’': 415,\n",
       "             'reason': 416,\n",
       "             'generic': 417,\n",
       "             'quit': 418,\n",
       "             'stuff': 419,\n",
       "             'chronic': 420,\n",
       "             'let': 421,\n",
       "             'nauseous': 422,\n",
       "             'wish': 423,\n",
       "             'lbs': 424,\n",
       "             'cough': 425,\n",
       "             'run': 426,\n",
       "             'miracle': 427,\n",
       "             'insert': 428,\n",
       "             'remove': 429,\n",
       "             'pack': 430,\n",
       "             'easy': 431,\n",
       "             'disorder': 432,\n",
       "             'develop': 433,\n",
       "             'along': 434,\n",
       "             'constant': 435,\n",
       "             'yet': 436,\n",
       "             'early': 437,\n",
       "             'such': 438,\n",
       "             'breast': 439,\n",
       "             'asleep': 440,\n",
       "             'drop': 441,\n",
       "             'diet': 442,\n",
       "             'cramping': 443,\n",
       "             'crazy': 444,\n",
       "             'positive': 445,\n",
       "             'major': 446,\n",
       "             'improve': 447,\n",
       "             'fast': 448,\n",
       "             'reaction': 449,\n",
       "             'pay': 450,\n",
       "             'although': 451,\n",
       "             'fatigue': 452,\n",
       "             'doc': 453,\n",
       "             'chest': 454,\n",
       "             'job': 455,\n",
       "             'person': 456,\n",
       "             'remember': 457,\n",
       "             'plan': 458,\n",
       "             'pregnancy': 459,\n",
       "             'under': 460,\n",
       "             'antibiotic': 461,\n",
       "             'stress': 462,\n",
       "             'er': 463,\n",
       "             'turn': 464,\n",
       "             'foot': 465,\n",
       "             'move': 466,\n",
       "             'part': 467,\n",
       "             'talk': 468,\n",
       "             'slight': 469,\n",
       "             'wear': 470,\n",
       "             'cycle': 471,\n",
       "             'please': 472,\n",
       "             'hospital': 473,\n",
       "             'knee': 474,\n",
       "             'brand': 475,\n",
       "             'everyday': 476,\n",
       "             'dizziness': 477,\n",
       "             'insertion': 478,\n",
       "             'add': 479,\n",
       "             'arm': 480,\n",
       "             'home': 481,\n",
       "             'dizzy': 482,\n",
       "             'withdrawal': 483,\n",
       "             'save': 484,\n",
       "             'constipation': 485,\n",
       "             'follow': 486,\n",
       "             'highly': 487,\n",
       "             'cover': 488,\n",
       "             'mind': 489,\n",
       "             'literally': 490,\n",
       "             'hot': 491,\n",
       "             'stick': 492,\n",
       "             '....': 493,\n",
       "             'regular': 494,\n",
       "             'decrease': 495,\n",
       "             'care': 496,\n",
       "             'treat': 497,\n",
       "             'four': 498,\n",
       "             'his': 499,\n",
       "             'improvement': 500,\n",
       "             'constantly': 501,\n",
       "             'quite': 502,\n",
       "             'sweat': 503,\n",
       "             'calm': 504,\n",
       "             '24': 505,\n",
       "             'either': 506,\n",
       "             'probably': 507,\n",
       "             'red': 508,\n",
       "             'ache': 509,\n",
       "             'type': 510,\n",
       "             'function': 511,\n",
       "             'am': 512,\n",
       "             'suggest': 513,\n",
       "             'might': 514,\n",
       "             'super': 515,\n",
       "             'place': 516,\n",
       "             'healthy': 517,\n",
       "             'pass': 518,\n",
       "             'wo': 519,\n",
       "             'anxious': 520,\n",
       "             'finish': 521,\n",
       "             'worry': 522,\n",
       "             'neck': 523,\n",
       "             'bc': 524,\n",
       "             'amount': 525,\n",
       "             'strong': 526,\n",
       "             'hate': 527,\n",
       "             'quickly': 528,\n",
       "             'cost': 529,\n",
       "             'huge': 530,\n",
       "             'plus': 531,\n",
       "             'brain': 532,\n",
       "             '\\n\\n\\n\\n': 533,\n",
       "             'smoke': 534,\n",
       "             'sugar': 535,\n",
       "             'realize': 536,\n",
       "             'itching': 537,\n",
       "             '’s': 538,\n",
       "             'rest': 539,\n",
       "             'child': 540,\n",
       "             'throw': 541,\n",
       "             'post': 542,\n",
       "             'struggle': 543,\n",
       "             'third': 544,\n",
       "             'joint': 545,\n",
       "             'husband': 546,\n",
       "             'dream': 547,\n",
       "             'kind': 548,\n",
       "             'miss': 549,\n",
       "             'age': 550,\n",
       "             'family': 551,\n",
       "             'school': 552,\n",
       "             'yes': 553,\n",
       "             'instead': 554,\n",
       "             'except': 555,\n",
       "             'entire': 556,\n",
       "             'between': 557,\n",
       "             'buy': 558,\n",
       "             '18': 559,\n",
       "             'friend': 560,\n",
       "             'area': 561,\n",
       "             'luck': 562,\n",
       "             'cut': 563,\n",
       "             'currently': 564,\n",
       "             'term': 565,\n",
       "             'emotional': 566,\n",
       "             'course': 567,\n",
       "             'hormone': 568,\n",
       "             'itch': 569,\n",
       "             'yeast': 570,\n",
       "             'prior': 571,\n",
       "             'apply': 572,\n",
       "             'someone': 573,\n",
       "             'especially': 574,\n",
       "             'her': 575,\n",
       "             'barely': 576,\n",
       "             'lb': 577,\n",
       "             'honestly': 578,\n",
       "             'return': 579,\n",
       "             'iud': 580,\n",
       "             'self': 581,\n",
       "             'glad': 582,\n",
       "             'suppose': 583,\n",
       "             'bipolar': 584,\n",
       "             'sit': 585,\n",
       "             'form': 586,\n",
       "             'trouble': 587,\n",
       "             'rate': 588,\n",
       "             '60': 589,\n",
       "             'expensive': 590,\n",
       "             '2nd': 591,\n",
       "             'totally': 592,\n",
       "             'their': 593,\n",
       "             'throat': 594,\n",
       "             'mean': 595,\n",
       "             'rather': 596,\n",
       "             'nt': 597,\n",
       "             'nightmare': 598,\n",
       "             'nearly': 599,\n",
       "             'bowel': 600,\n",
       "             'hear': 601,\n",
       "             'bathroom': 602,\n",
       "             'often': 603,\n",
       "             'got': 604,\n",
       "             'health': 605,\n",
       "             'include': 606,\n",
       "             'non': 607,\n",
       "             'weigh': 608,\n",
       "             'bottle': 609,\n",
       "             '1/2': 610,\n",
       "             'weird': 611,\n",
       "             'must': 612,\n",
       "             'scared': 613,\n",
       "             'guess': 614,\n",
       "             'nose': 615,\n",
       "             'check': 616,\n",
       "             'rash': 617,\n",
       "             'straight': 618,\n",
       "             'late': 619,\n",
       "             'intense': 620,\n",
       "             'fact': 621,\n",
       "             'expect': 622,\n",
       "             'focus': 623,\n",
       "             'prevent': 624,\n",
       "             '1st': 625,\n",
       "             'pimple': 626,\n",
       "             'ok': 627,\n",
       "             'pharmacy': 628,\n",
       "             'consider': 629,\n",
       "             'yesterday': 630,\n",
       "             'slowly': 631,\n",
       "             'case': 632,\n",
       "             'baby': 633,\n",
       "             'woman': 634,\n",
       "             'subside': 635,\n",
       "             'active': 636,\n",
       "             'real': 637,\n",
       "             '3rd': 638,\n",
       "             'forget': 639,\n",
       "             'vomit': 640,\n",
       "             'condition': 641,\n",
       "             '14': 642,\n",
       "             'unfortunately': 643,\n",
       "             '150': 644,\n",
       "             'min': 645,\n",
       "             'burning': 646,\n",
       "             'hopefully': 647,\n",
       "             'system': 648,\n",
       "             'wonderful': 649,\n",
       "             'kid': 650,\n",
       "             'xanax': 651,\n",
       "             'possible': 652,\n",
       "             '16': 653,\n",
       "             'anti': 654,\n",
       "             'six': 655,\n",
       "             'craving': 656,\n",
       "             'zoloft': 657,\n",
       "             'breakout': 658,\n",
       "             'yr': 659,\n",
       "             'normally': 660,\n",
       "             'awake': 661,\n",
       "             'research': 662,\n",
       "             'son': 663,\n",
       "             'allergy': 664,\n",
       "             'patient': 665,\n",
       "             'god': 666,\n",
       "             'rid': 667,\n",
       "             'lexapro': 668,\n",
       "             'antidepressant': 669,\n",
       "             'sleepy': 670,\n",
       "             'slow': 671,\n",
       "             'sinus': 672,\n",
       "             'episode': 673,\n",
       "             'vision': 674,\n",
       "             'swell': 675,\n",
       "             'own': 676,\n",
       "             'kick': 677,\n",
       "             'implant': 678,\n",
       "             'vomiting': 679,\n",
       "             'mention': 680,\n",
       "             'nerve': 681,\n",
       "             '11': 682,\n",
       "             'write': 683,\n",
       "             'show': 684,\n",
       "             'hungry': 685,\n",
       "             '300': 686,\n",
       "             'ear': 687,\n",
       "             'close': 688,\n",
       "             'option': 689,\n",
       "             'allow': 690,\n",
       "             'moody': 691,\n",
       "             'sensitive': 692,\n",
       "             'serious': 693,\n",
       "             'eventually': 694,\n",
       "             'beginning': 695,\n",
       "             'upset': 696,\n",
       "             'none': 697,\n",
       "             'allergic': 698,\n",
       "             'evening': 699,\n",
       "             'five': 700,\n",
       "             'total': 701,\n",
       "             'flash': 702,\n",
       "             'dermatologist': 703,\n",
       "             'size': 704,\n",
       "             'seizure': 705,\n",
       "             'hit': 706,\n",
       "             'die': 707,\n",
       "             'zero': 708,\n",
       "             'difficult': 709,\n",
       "             'boyfriend': 710,\n",
       "             'minor': 711,\n",
       "             'complete': 712,\n",
       "             'cure': 713,\n",
       "             'bp': 714,\n",
       "             'lower': 715,\n",
       "             'slightly': 716,\n",
       "             'procedure': 717,\n",
       "             '200': 718,\n",
       "             'relieve': 719,\n",
       "             'bring': 720,\n",
       "             'wrong': 721,\n",
       "             'send': 722,\n",
       "             'itchy': 723,\n",
       "             'breath': 724,\n",
       "             'okay': 725,\n",
       "             'meal': 726,\n",
       "             'single': 727,\n",
       "             'unbearable': 728,\n",
       "             'avoid': 729,\n",
       "             'alcohol': 730,\n",
       "             'spend': 731,\n",
       "             'suicidal': 732,\n",
       "             'disease': 733,\n",
       "             'adhd': 734,\n",
       "             'bone': 735,\n",
       "             'cystic': 736,\n",
       "             'uncomfortable': 737,\n",
       "             'nervous': 738,\n",
       "             'unable': 739,\n",
       "             'state': 740,\n",
       "             'clean': 741,\n",
       "             'throughout': 742,\n",
       "             'benefit': 743,\n",
       "             'uti': 744,\n",
       "             'stand': 745,\n",
       "             'adjust': 746,\n",
       "             'cancer': 747,\n",
       "             'worried': 748,\n",
       "             'flu': 749,\n",
       "             '90': 750,\n",
       "             'b': 751,\n",
       "             'large': 752,\n",
       "             'irritable': 753,\n",
       "             'mental': 754,\n",
       "             '500': 755,\n",
       "             '75': 756,\n",
       "             'our': 757,\n",
       "             'truly': 758,\n",
       "             'figure': 759,\n",
       "             'usual': 760,\n",
       "             'lip': 761,\n",
       "             'disappear': 762,\n",
       "             'manage': 763,\n",
       "             'money': 764,\n",
       "             'combination': 765,\n",
       "             'left': 766,\n",
       "             'discomfort': 767,\n",
       "             'shoulder': 768,\n",
       "             'occasional': 769,\n",
       "             'miserable': 770,\n",
       "             'receive': 771,\n",
       "             'flare': 772,\n",
       "             'm': 773,\n",
       "             'depo': 774,\n",
       "             'comment': 775,\n",
       "             'company': 776,\n",
       "             'spasm': 777,\n",
       "             'female': 778,\n",
       "             'movement': 779,\n",
       "             'mostly': 780,\n",
       "             'bloat': 781,\n",
       "             'name': 782,\n",
       "             'easily': 783,\n",
       "             'minimal': 784,\n",
       "             'anyway': 785,\n",
       "             'compare': 786,\n",
       "             'fibromyalgia': 787,\n",
       "             'syndrome': 788,\n",
       "             'house': 789,\n",
       "             'story': 790,\n",
       "             'fever': 791,\n",
       "             'gel': 792,\n",
       "             'angry': 793,\n",
       "             'enjoy': 794,\n",
       "             'swollen': 795,\n",
       "             'discharge': 796,\n",
       "             'grow': 797,\n",
       "             '2015': 798,\n",
       "             'matter': 799,\n",
       "             'arthritis': 800,\n",
       "             'social': 801,\n",
       "             'x': 802,\n",
       "             'lack': 803,\n",
       "             'top': 804,\n",
       "             'price': 805,\n",
       "             'memory': 806,\n",
       "             'mirena': 807,\n",
       "             'sad': 808,\n",
       "             'shake': 809,\n",
       "             'libido': 810,\n",
       "             'handle': 811,\n",
       "             'daughter': 812,\n",
       "             'afraid': 813,\n",
       "             'initially': 814,\n",
       "             'wellbutrin': 815,\n",
       "             'discontinue': 816,\n",
       "             'prep': 817,\n",
       "             '80': 818,\n",
       "             'fear': 819,\n",
       "             'irregular': 820,\n",
       "             'prozac': 821,\n",
       "             'perfect': 822,\n",
       "             'quality': 823,\n",
       "             'number': 824,\n",
       "             'mess': 825,\n",
       "             'therapy': 826,\n",
       "             'inside': 827,\n",
       "             'moderate': 828,\n",
       "             'asthma': 829,\n",
       "             'breathe': 830,\n",
       "             'psychiatrist': 831,\n",
       "             'multiple': 832,\n",
       "             'cyst': 833,\n",
       "             'middle': 834,\n",
       "             'scare': 835,\n",
       "             'lay': 836,\n",
       "             'smell': 837,\n",
       "             '13': 838,\n",
       "             'idea': 839,\n",
       "             'open': 840,\n",
       "             'tooth': 841,\n",
       "             'bloated': 842,\n",
       "             'bladder': 843,\n",
       "             'hell': 844,\n",
       "             'weak': 845,\n",
       "             '--': 846,\n",
       "             'appointment': 847,\n",
       "             'wash': 848,\n",
       "             'complaint': 849,\n",
       "             'mine': 850,\n",
       "             'ready': 851,\n",
       "             'process': 852,\n",
       "             'quick': 853,\n",
       "             'basically': 854,\n",
       "             '45': 855,\n",
       "             '2016': 856,\n",
       "             'tri': 857,\n",
       "             'kidney': 858,\n",
       "             'kill': 859,\n",
       "             'effexor': 860,\n",
       "             'hormonal': 861,\n",
       "             'speak': 862,\n",
       "             'abdominal': 863,\n",
       "             'update': 864,\n",
       "             'march': 865,\n",
       "             'physical': 866,\n",
       "             'colonoscopy': 867,\n",
       "             'sensation': 868,\n",
       "             'understand': 869,\n",
       "             'bedtime': 870,\n",
       "             'hardly': 871,\n",
       "             '+': 872,\n",
       "             'hold': 873,\n",
       "             'damage': 874,\n",
       "             'drowsy': 875,\n",
       "             'ten': 876,\n",
       "             'finger': 877,\n",
       "             'acid': 878,\n",
       "             'market': 879,\n",
       "             'general': 880,\n",
       "             'extra': 881,\n",
       "             'lo': 882,\n",
       "             'nurse': 883,\n",
       "             'room': 884,\n",
       "             'sample': 885,\n",
       "             '17': 886,\n",
       "             'hip': 887,\n",
       "             'helpful': 888,\n",
       "             'bump': 889,\n",
       "             'lead': 890,\n",
       "             'breakthrough': 891,\n",
       "             'list': 892,\n",
       "             'ibuprofen': 893,\n",
       "             'ring': 894,\n",
       "             'loose': 895,\n",
       "             'site': 896,\n",
       "             'order': 897,\n",
       "             'exactly': 898,\n",
       "             'stool': 899,\n",
       "             'visit': 900,\n",
       "             'cheek': 901,\n",
       "             'chill': 902,\n",
       "             'dark': 903,\n",
       "             'release': 904,\n",
       "             'wife': 905,\n",
       "             'emotion': 906,\n",
       "             'watch': 907,\n",
       "             'tomorrow': 908,\n",
       "             'choice': 909,\n",
       "             'similar': 910,\n",
       "             'previously': 911,\n",
       "             'careful': 912,\n",
       "             'near': 913,\n",
       "             'share': 914,\n",
       "             'cymbalta': 915,\n",
       "             'numb': 916,\n",
       "             'significant': 917,\n",
       "             'nice': 918,\n",
       "             'suck': 919,\n",
       "             'chance': 920,\n",
       "             'gas': 921,\n",
       "             'sexual': 922,\n",
       "             'chin': 923,\n",
       "             'alone': 924,\n",
       "             'deep': 925,\n",
       "             'fight': 926,\n",
       "             'counter': 927,\n",
       "             'fail': 928,\n",
       "             'heal': 929,\n",
       "             'noticeable': 930,\n",
       "             'scary': 931,\n",
       "             'risk': 932,\n",
       "             'situation': 933,\n",
       "             'liver': 934,\n",
       "             'ambien': 935,\n",
       "             'success': 936,\n",
       "             'sweating': 937,\n",
       "             'june': 938,\n",
       "             'tolerate': 939,\n",
       "             'touch': 940,\n",
       "             'learn': 941,\n",
       "             'male': 942,\n",
       "             'tolerance': 943,\n",
       "             'seriously': 944,\n",
       "             'relate': 945,\n",
       "             'restless': 946,\n",
       "             'oh': 947,\n",
       "             'saver': 948,\n",
       "             'frequent': 949,\n",
       "             'medical': 950,\n",
       "             'young': 951,\n",
       "             'clot': 952,\n",
       "             'ease': 953,\n",
       "             'previous': 954,\n",
       "             'round': 955,\n",
       "             'together': 956,\n",
       "             'july': 957,\n",
       "             ':)': 958,\n",
       "             'yrs': 959,\n",
       "             'hive': 960,\n",
       "             'tear': 961,\n",
       "             'awesome': 962,\n",
       "             'push': 963,\n",
       "             'otherwise': 964,\n",
       "             'c': 965,\n",
       "             'significantly': 966,\n",
       "             '4th': 967,\n",
       "             'waste': 968,\n",
       "             'adderall': 969,\n",
       "             'january': 970,\n",
       "             'wean': 971,\n",
       "             'redness': 972,\n",
       "             'seroquel': 973,\n",
       "             'method': 974,\n",
       "             'white': 975,\n",
       "             'regulate': 976,\n",
       "             'advise': 977,\n",
       "             'inhaler': 978,\n",
       "             '21': 979,\n",
       "             'upper': 980,\n",
       "             'react': 981,\n",
       "             'ocd': 982,\n",
       "             'force': 983,\n",
       "             'lady': 984,\n",
       "             'beat': 985,\n",
       "             'knock': 986,\n",
       "             'monistat': 987,\n",
       "             'toilet': 988,\n",
       "             'advice': 989,\n",
       "             'disc': 990,\n",
       "             'mom': 991,\n",
       "             'paxil': 992,\n",
       "             'relationship': 993,\n",
       "             'safe': 994,\n",
       "             'vagina': 995,\n",
       "             'flow': 996,\n",
       "             'pull': 997,\n",
       "             'till': 998,\n",
       "             'lie': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = json.dumps(counter_reviews)\n",
    "with open('full_vocab_lemmas.json', 'w') as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grasser Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 examples completed\n",
      "10000 examples completed\n",
      "20000 examples completed\n",
      "30000 examples completed\n",
      "40000 examples completed\n",
      "50000 examples completed\n",
      "60000 examples completed\n",
      "70000 examples completed\n",
      "80000 examples completed\n",
      "90000 examples completed\n",
      "100000 examples completed\n",
      "110000 examples completed\n",
      "120000 examples completed\n",
      "130000 examples completed\n"
     ]
    }
   ],
   "source": [
    "counter_reviews = Counter()\n",
    "lemma_vocab = create_vocab_lemma(f\"{DATA_PATH}/grasser_data/train_grasser_data.csv\", \n",
    "                            counter_reviews, 100)\n",
    "s = json.dumps(counter_reviews)\n",
    "with open('grasser_vocab_lemmas.json', 'w') as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmas\n",
      "['they', 'say', 'that', 'he', 'would', 'say', 'to', 'she', 'what', 'she', 'say']\n",
      "\n",
      "tags\n",
      "['PRP', 'VBD', 'IN', 'PRP', 'MD', 'VB', 'IN', 'PRP', 'WP', 'PRP', 'VBD']\n",
      "\n",
      "part of speech\n",
      "['PRON', 'VERB', 'SCONJ', 'PRON', 'AUX', 'VERB', 'ADP', 'PRON', 'PRON', 'PRON', 'VERB']\n",
      "\n",
      "dependencies\n",
      "['nsubj', 'ROOT', 'mark', 'nsubj', 'aux', 'ccomp', 'prep', 'pobj', 'ccomp', 'nsubj', 'ROOT']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"They said that he would say to her what she said\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "print(\"lemmas\")\n",
    "print([token.lemma_ for token in doc])\n",
    "print()\n",
    "print(\"tags\")\n",
    "print([token.tag_ for token in doc])\n",
    "print()\n",
    "print(\"part of speech\")\n",
    "print([token.pos_ for token in doc])\n",
    "print()\n",
    "print(\"dependencies\")\n",
    "print([token.dep_ for token in doc])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
